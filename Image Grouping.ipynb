{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import rawpy\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import ipdb\n",
    "import os\n",
    "import platform\n",
    "import multiprocessing\n",
    "import json\n",
    "from functools import lru_cache\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = namedtuple('Image', 'icon features filename timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insensitive_glob(pattern):\n",
    "    def either(c):\n",
    "        return '[%s%s]' % (c.lower(), c.upper()) if c.isalpha() else c\n",
    "    return glob.glob(''.join(map(either, pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, vector_size=32):\n",
    "    try:\n",
    "        # Using KAZE, cause SIFT, ORB and other was moved to additional module\n",
    "        # which is adding addtional pain during install\n",
    "        alg = cv2.KAZE_create()\n",
    "        # Dinding image keypoints\n",
    "        kps = alg.detect(image)\n",
    "        # Getting first 32 of them. \n",
    "        # Number of keypoints is varies depend on image size and color pallet\n",
    "        # Sorting them based on keypoint response value(bigger is better)\n",
    "        kps = sorted(kps, key=lambda x: -x.response)[:vector_size]\n",
    "        # computing descriptors vector\n",
    "        kps, dsc = alg.compute(image, kps)\n",
    "        # Flatten all of them in one big vector - our feature vector\n",
    "        dsc = dsc.flatten()\n",
    "        # Making descriptor of same size\n",
    "        # Descriptor vector size is 64\n",
    "        needed_size = (vector_size * 64)\n",
    "        if dsc.size < needed_size:\n",
    "            # if we have less the 32 descriptors then just adding zeros at the\n",
    "            # end of our feature vector\n",
    "            dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n",
    "    except cv2.error as e:\n",
    "        print('Error: ', e)\n",
    "        return None\n",
    "\n",
    "    return dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_distance(point, other_points, distance_function):\n",
    "  return sorted(other_points, key=lambda p: distance_function(point, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "class AugmentedPoint:\n",
    "  def __init__(self, point, cluster):\n",
    "    self.point = point\n",
    "    self.cluster = cluster\n",
    "\n",
    "def cluster_images_with_dbscan(points, distance_function, e):\n",
    "  # See https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68#4591\n",
    "  if len(points) == 0:\n",
    "    return []\n",
    "\n",
    "  next_cluster_number = 0\n",
    "  augmented_points = [AugmentedPoint(point=point, cluster=-1) for point in points]\n",
    "\n",
    "  # While there are still points to consider\n",
    "  for current_point in augmented_points:\n",
    "\n",
    "    # Sort all other points by distance, excluding current point\n",
    "    closest_points = sort_by_distance(current_point, augmented_points, lambda a, b: distance_function(a.point, b.point))[1:]\n",
    "\n",
    "    # Drop all points too far away to join\n",
    "    viable_points = list(filter(lambda another_point: distance_function(current_point.point, another_point.point) <= e, closest_points))\n",
    "    \n",
    "    if len(viable_points) == 0:\n",
    "      # Too far away to join anything. Add into its own cluster.\n",
    "      current_point.cluster = next_cluster_number\n",
    "      next_cluster_number += 1\n",
    "    else:\n",
    "      # Found points to join with!\n",
    "      for point_to_join in viable_points:\n",
    "          if point_to_join.cluster == -1:\n",
    "            # If the close point does not have a cluster\n",
    "            \n",
    "            if current_point.cluster != -1:\n",
    "                # If we're already part of a cluster, absorb point\n",
    "                point_to_join.cluster = current_point.cluster\n",
    "            else:\n",
    "                # If we're not part of a cluster, create a new cluster with point\n",
    "                current_point.cluster = next_cluster_number\n",
    "                point_to_join.cluster = next_cluster_number\n",
    "                next_cluster_number += 1\n",
    "          else:\n",
    "            # If the point is already part of a cluster\n",
    "          \n",
    "            if point_to_join.cluster == current_point.cluster:\n",
    "                # Already part of the same group, feel free to skip\n",
    "                pass\n",
    "            elif current_point.cluster != -1:\n",
    "                # If current point is already part of cluster, merge point's cluster into the new cluster\n",
    "                for point_in_current_cluster in filter(lambda p: p.cluster == current_point.cluster, augmented_points):\n",
    "                  point_in_current_cluster.cluster = point_to_join.cluster\n",
    "            else:\n",
    "                # If the point is not part of a cluster, join that cluster\n",
    "                current_point.cluster = point_to_join.cluster\n",
    "    \n",
    "  # All points have now been visited and joined together. Group by cluster id and return.\n",
    "  clusters = [[] for _ in range(next_cluster_number)]\n",
    "  for augmented_point in augmented_points:\n",
    "    clusters[augmented_point.cluster].append(augmented_point.point)\n",
    "  \n",
    "  return clusters\n",
    "\n",
    "\n",
    "def test_cluster_images_with_dbscan():\n",
    "    def distance_function(a, b):\n",
    "        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n",
    "    \n",
    "    # Should return empty list if no points\n",
    "    assert(0 == len(cluster_images_with_dbscan([], distance_function, 5)))\n",
    "    \n",
    "    # Should add one point to one group\n",
    "    assert([[(0, 0)]] == cluster_images_with_dbscan([(0, 0)], distance_function, 5))\n",
    "    \n",
    "    # Should add a second point to the same group\n",
    "    assert([[(0, 0), (1, 1)]] == cluster_images_with_dbscan([(0, 0), (1, 1)], distance_function, 5))\n",
    "    \n",
    "    # Should not add a second point if it's too far\n",
    "    assert([[(0, 0)], [(1, 1)]] == cluster_images_with_dbscan([(0, 0), (1, 1)], distance_function, 1))\n",
    "    \n",
    "    # Should find three groups\n",
    "    points = [(0, 0), (1, 1), (-1, -1),\n",
    "              (20, 20), (19, 19), (21, 21),\n",
    "              (-20, -20), (-21, -21), (-19, -19)]\n",
    "    expected = [[(0, 0), (1, 1), (-1, -1)],\n",
    "                [(20, 20), (19, 19), (21, 21)],\n",
    "                [(-20, -20), (-21, -21), (-19, -19)]]\n",
    "    assert(expected == cluster_images_with_dbscan(points, distance_function, 2))\n",
    "    \n",
    "    # Should handle two sub-clusters joining together\n",
    "    points = [(4, 4), (1, 1), (0, 0)]\n",
    "    expected = [[(4, 4), (1, 1), (0, 0)]]\n",
    "    assert(expected == cluster_images_with_dbscan(points, distance_function, 10))\n",
    "    \n",
    "    # Should corrently handle merging two full groups\n",
    "    points = [(0.48, 0.69), (0.53, 0.73), (0, 0), (0.35, 0)]\n",
    "    expected = [[(0.48, 0.69), (0.53, 0.73), (0, 0), (0.35, 0)]]\n",
    "    assert(expected == cluster_images_with_dbscan(points, distance_function, 10))\n",
    "    \n",
    "    print(\"Tests passed!\")\n",
    "    \n",
    "test_cluster_images_with_dbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_date(path_to_file):\n",
    "    \"\"\"\n",
    "    Try to get the date that a file was created, falling back to when it was\n",
    "    last modified if that isn't possible.\n",
    "    See http://stackoverflow.com/a/39501288/1709587 for explanation.\n",
    "    \"\"\"\n",
    "    if platform.system() == 'Windows':\n",
    "        return os.path.getctime(path_to_file)\n",
    "    else:\n",
    "        stat = os.stat(path_to_file)\n",
    "        try:\n",
    "            return stat.st_birthtime\n",
    "        except AttributeError:\n",
    "            # We're probably on Linux. No easy way to get creation dates here,\n",
    "            # so we'll settle for when its content was last modified.\n",
    "            return stat.st_mtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "image_directory = 'test/resources'\n",
    "image_filenames = insensitive_glob(path.join(image_directory, \"*.arw\"))\n",
    "\n",
    "#image_directory = '/media/anima/Auxillary/Camera Photos/20.02.2019'\n",
    "#image_filenames = insensitive_glob(path.join(image_directory, \"*.arw\"))[:200]\n",
    "\n",
    "print(\"Loading images\")\n",
    "\n",
    "images = []\n",
    "\n",
    "cpu_pool = multiprocessing.Pool(multiprocessing.cpu_count() * 2)\n",
    "\n",
    "def load_image(image_filename):\n",
    "    raw_image = rawpy.imread(image_filename)\n",
    "    rgb_image = raw_image.postprocess(half_size=True, use_camera_wb=True, output_bps=8)\n",
    "\n",
    "    icon = cv2.resize(rgb_image, (100, 100))\n",
    "    features = icon\n",
    "    modified_time = creation_date(image_filename)\n",
    "\n",
    "    return Image(icon, features, image_filename, timestamp=modified_time)\n",
    "\n",
    "# images = cpu_pool.map(load_image, image_filenames)\n",
    "\n",
    "images = list(map(load_image, image_filenames))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(clusters):\n",
    "    i = 1\n",
    "    for cluster in clusters:\n",
    "      print(\"Cluster %s: \" % i)\n",
    "      for item in cluster:\n",
    "        print(\"  - %s\" % item.filename)\n",
    "      i = i + 1\n",
    "    \n",
    "%matplotlib qt\n",
    "def visualise_clusters(clusters):\n",
    "    image_size = clusters[0][0].icon.shape[0]\n",
    "    number_of_images = np.sum([len(cluster) for cluster in clusters])\n",
    "    hr_height = 25\n",
    "    vr_length = 25\n",
    "    \n",
    "    max_images_per_column = 12\n",
    "    columns = int(number_of_images / max_images_per_column) + 1\n",
    "    \n",
    "    width = (image_size + vr_length) * columns\n",
    "    height = image_size * number_of_images + hr_height * len(clusters)\n",
    "    buffer = np.zeros((height, width, 3))\n",
    "    \n",
    "    i = 0\n",
    "    hr_offset = 0\n",
    "    for cluster in clusters:\n",
    "        for iImage, image in enumerate(cluster):\n",
    "            column = int(i / max_images_per_column)\n",
    "            if column != int((i - 1) / max_images_per_column):\n",
    "                if iImage == 0:\n",
    "                    hr_offset = hr_height\n",
    "                else:\n",
    "                    hr_offset = 0\n",
    "            row_offset = i * image_size + hr_offset - column * max_images_per_column * image_size\n",
    "            column_offset = int(i / max_images_per_column) * image_size\n",
    "            buffer[row_offset:row_offset + image_size,column_offset:column_offset + image_size,:] = image.icon\n",
    "            i = i + 1\n",
    "        hr_offset = hr_offset + hr_height\n",
    "    cv2.imshow(\"Clusters\", buffer / np.max(buffer))\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does't work well because not normalised\n",
    "def image_distance_mse(a, b):\n",
    "    return np.sum((a.features - b.features)**2)\n",
    "\n",
    "# Pretty good\n",
    "def image_distance_mse_sum_norm(a, b):\n",
    "        return np.sum((a.features / np.sum(a.features) - b.features / np.sum(b.features))**2)\n",
    "\n",
    "# Not very good, not properly normalisied\n",
    "def image_distance_mse_mean_norm(a, b):\n",
    "    return np.sum((a.features / np.mean(a.features) - b.features / np.mean(b.features))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering images\n",
      "Cluster 1: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02198.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02199.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02200.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02201.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02204.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02205.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02206.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02207.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02208.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02209.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02210.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02211.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02212.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02213.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02214.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02215.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02216.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02255.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02256.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02257.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02258.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02261.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02262.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02275.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02276.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02277.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02278.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02279.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02280.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02294.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02295.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02296.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02297.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02298.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02299.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02300.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02301.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02302.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02313.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02314.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02315.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02316.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02317.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02318.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02319.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02320.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02321.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02322.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02323.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02324.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02325.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02327.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02328.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02329.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02330.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02331.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02332.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02333.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02334.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02335.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02336.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02337.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02338.ARW\n",
      "Cluster 2: \n",
      "Cluster 3: \n",
      "Cluster 4: \n",
      "Cluster 5: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02259.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02260.ARW\n",
      "Cluster 6: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02263.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02264.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02265.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02266.ARW\n",
      "Cluster 7: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02267.ARW\n",
      "Cluster 8: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02268.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02269.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02270.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02271.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02273.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02274.ARW\n",
      "Cluster 9: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02281.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02282.ARW\n",
      "Cluster 10: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02283.ARW\n",
      "Cluster 11: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02284.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02285.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02286.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02287.ARW\n",
      "Cluster 12: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02288.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02289.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02291.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02292.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02293.ARW\n",
      "Cluster 13: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02303.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02304.ARW\n",
      "Cluster 14: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02305.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02306.ARW\n",
      "Cluster 15: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02307.ARW\n",
      "Cluster 16: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02309.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02310.ARW\n",
      "Cluster 17: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02311.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02312.ARW\n",
      "Cluster 18: \n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02339.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02340.ARW\n",
      "  - /media/anima/Auxillary/Camera Photos/20.02.2019/DSC02341.ARW\n"
     ]
    }
   ],
   "source": [
    "print(\"Clustering images\")\n",
    "\n",
    "a =0.00002\n",
    "images_to_group = images\n",
    "\n",
    "# print([(str(i) + \" - \" + image.filename) for i, image in enumerate(images_to_group)])\n",
    "\n",
    "print_clusters(cluster_images_with_dbscan(images_to_group, image_distance_mse_sum_norm, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419.32858419666303\n",
      "817.5009533159625\n",
      "864.4394283491508\n",
      "583.5984933187045\n",
      "691.2085125261216\n",
      "199.09619596457537\n",
      "[0.40658093 0.84838394 0.75210641 1.         0.8985156  0.35053875]\n"
     ]
    }
   ],
   "source": [
    "print(image_distance_mse_mean_norm(images[0], images[1]))\n",
    "print(image_distance_mse_mean_norm(images[0], images[2]))\n",
    "print(image_distance_mse_mean_norm(images[0], images[3]))\n",
    "print(image_distance_mse_mean_norm(images[1], images[2]))\n",
    "print(image_distance_mse_mean_norm(images[1], images[3]))\n",
    "print(image_distance_mse_mean_norm(images[2], images[3]))\n",
    "\n",
    "a = np.array(  [22594.711631993814,\n",
    "                47146.801882724394,\n",
    "                41796.4206178841,\n",
    "                55572.482975170744,\n",
    "                49932.742614843504,\n",
    "                19480.308860878984])\n",
    "\n",
    "a = a/np.max(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(images, key=lambda x: x.timestamp)\n",
    "\n",
    "difference_threshold = 0.00002\n",
    "difference_function = image_distance_mse_sum_norm\n",
    "\n",
    "differences = np.array([])\n",
    "for i in range(len(images) - 1):\n",
    "    differences = np.append(differences, (difference_function(images[i], images[i + 1])))\n",
    "\n",
    "breaking_points = np.append([0], differences > difference_threshold)\n",
    "groups = np.cumsum(breaking_points)\n",
    "grouped_images = [[] for _ in range((groups[-1] + 1))]\n",
    "for i, group in enumerate(groups):\n",
    "    grouped_images[group] = list(grouped_images[group]) + [images[i]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_clusters(grouped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "textified_groups = list(map(lambda group: list(map(lambda image: image.filename, group)), grouped_images))\n",
    "\n",
    "with open('grouped_images.json', 'w') as outfile:\n",
    "    json.dump(textified_groups, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
